---
title: "Methods 3 - Assignment 4"
author: "Esteban David Maidana"
date: "09/02/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## Required Packages 
## install.packages("rethinking") ##Installed
## install.packages("slider") ##Installed
## install.packages("plotly") ##Installed
## install.packages("dplyr") ##Installed
## install.packages("ggplot2") ##Installed
## install.packages("skimr") ##Installed

## Required libraries
library(rethinking) ##Loaded
library(tidyverse) ##Loaded
library(dplyr) ##Loaded
library(ggplot2) ##Loaded
library(skimr) ##Loaded
library(lme4) ##Loaded
library(data.table) ##Loaded
library(nlme) ##Loaded

## LOAD DATA
## Get working directory
setwd('/Users/estebandavidmaidana/Desktop')
working_directory = getwd()
```

## Assignment 4: Dimensionality reduction; finding the signal among the noise.

## Exercises and objectives:

1) Use principal component analysis to improve the classification of subjective experience  
2) Use logistic regression with cross-validation to find the optimal number of principal components  

## REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__). 

## REMEMBER: This is Assignment 4 and will be part of your final portfolio.   

## Exercise 1: Use principal component analysis to improve the classification of subjective experience.

We will use the same files as we did in Assignment 3
The files `megmag_data.npy` and `pas_vector.npy` can be downloaded here (http://laumollerandersen.org/data_methods_3/megmag_data.npy) and here (http://laumollerandersen.org/data_methods_3/pas_vector.npy)  
The function `equalize_targets` is supplied - this time, we will only work with an equalized data set. One motivation for this is that we have a well-defined chance level that we can compare against. Furthermore, we will look at a single time point to decrease the dimensionality of the problem  

1) Create a covariance matrix, find the eigenvectors and the eigenvalues.
  i. Load `megmag_data.npy` and call it `data` using `np.load`. You can use `join`, which can be imported from `os.path`, to create paths from different string segments 
```{r}

```

  ii. Equalize the number of targets in `y` and `data` using `equalize_targets` 
```{r}

```

  iii. Construct `times=np.arange(-200, 804, 4)` and find the index corresponding to 248 ms - then reduce the dimensionality of `data` from three to two dimensions by only choosing the time index corresponding to 248 ms (248 ms was where we found the maximal average response in Assignment 3) 
```{r}

```

  iv. Scale the data using `StandardScaler`  
```{r}

```

  v. Calculate the sample covariance matrix for the sensors (you can use `np.cov`) and plot it (either using `plt.imshow` or `sns.heatmap` (`import seaborn as sns`))  
```{r}

```

  vi. What does the off-diagonal activation imply about the independence of the signals measured by the 102 sensors?
```{r}

```

  vii. Run `np.linalg.matrix_rank` on the covariance matrix - what integer value do you get? (we'll use this later) 
```{r}

```

  viii. Find the eigenvalues and eigenvectors of the covariance matrix using `np.linalg.eig` - note that some of the numbers returned are complex numbers, consisting of a real and an imaginary part (they have a _j_ next to them). We are going to ignore this by only looking at the real parts of the eigenvectors and -values. Use `np.real` to retrieve only the real parts 
```{r}

```
    
2) Create the weighting matrix $W$ and the projected data, $Z$
  i. We need to sort the eigenvectors and eigenvalues according to the absolute values of the eigenvalues (use `np.abs` on the eigenvalues).  
```{r}

```
    
  ii. Then, we will find the correct ordering of the indices and create an array, e.g. `sorted_indices` that contains these indices. We want to sort the values from highest to lowest. For that, use `np.argsort`, which will find the indices that correspond to sorting the values from lowest to highest. Subsequently, use `np.flip`, which will reverse the order of the indices.
```{r}

```
  
  iii. Finally, create arrays of sorted eigenvalues and eigenvectors using the `sorted_indices` array just created. For the eigenvalues, it should like this `eigenvalues = eigenvalues[sorted_indices]` and for the eigenvectors: `eigenvectors = eigenvectors[:, sorted_indices]`
```{r}

```
  
  iv. Plot the log, `np.log`, of the eigenvalues, `plt.plot(np.log(eigenvalues), 'o')` - are there some values that stand out from the rest? In fact, 5 (noise) dimensions have already been projected out of the data - how does that relate to the matrix rank (Exercise 1.1.vii)  
```{r}

```
  
  v. Create the weighting matrix, `W` (it is the sorted eigenvectors)  
```{r}

```
  
  vi. Create the projected data, `Z`, $Z = XW$ - (you can check you did everything right by checking whether the $X$ you get from $X = ZW^T$ is equal to your original $X$, `np.isclose` may be of help)
```{r}

```
  
  vii. Create a new covariance matrix of the principal components (n=102) - plot it! What has happened off-diagonal and why?
```{r}

```

## Exercise 2: Use logistic regression with cross-validation to find the optimal number of principal components.

1) We are going to run logistic regression with in-sample validation 
  i. First, run standard logistic regression (no regularization) based on ~~$Z_{d \times k}$~~ and $Z_{n \times k}$ `y` (the target vector). Fit (`.fit`) 102 models based on: $k = [1, 2, ..., 101, 102]$ and $d = 102$. For each fit get the classification accuracy, (`.score`), when applied to ~~$Z_{d \times k}$~~ and $Z_{n \times k}$ and $y$. This is an in-sample validation. Use the solver `newton-cg` if the default solver doesn't converge
```{r}

```
  
  ii. Make a plot with the number of principal components on the _x_-axis and classification accuracy on the _y_-axis - what is the general trend and why is this so?
```{r}

```
  
  iii. In terms of classification accuracy, what is the effect of adding the five last components? Why do you think this is so?
```{r}

```
  

2) Now, we are going to use cross-validation - we are using `cross_val_score` and `StratifiedKFold` from `sklearn.model_selection`
  i. Define the variable: `cv = StratifiedKFold()` and run `cross_val_score` (remember to set the `cv` argument to your created `cv` variable). Use the same `estimator` in `cross_val_score` as in Exercise 2.1.i. Find the mean score over the 5 folds (the default of `StratifiedKFold`) for each $k$, $k = [1, 2, ..., 101, 102]$  
```{r}

```
  
  ii. Make a plot with the number of principal components on the _x_-axis and classification accuracy on the _y_-axis - how is this plot different from the one in Exercise 2.1.ii?
```{r}

```
  
  iii. What is the number of principal components, $k_{max\_accuracy}$, that results in the greatest classification accuracy when cross-validated?  
```{r}

```
  
  iv. How many percentage points is the classification accuracy increased with relative to the to the full-dimensional, $d$, dataset
```{r}

```
  
  v. How do the analyses in Exercises 2.1 and 2.2 differ from one another? Make sure to comment on the differences in optimization criteria.  
```{r}

```

3) We now make the assumption that $k_{max\_accuracy}$ is representative for each time sample (we only tested for 248 ms). We will use the PCA implementation from _scikit-learn_, i.e. import `PCA` from `sklearn.decomposition`.
  i. For __each__ of the 251 time samples, use the same estimator and cross-validation as in Exercises 2.1.i and 2.2.i. Run two analyses - one where you reduce the dimensionality to $k_{max\_accuracy}$ dimensions using `PCA` and one where you use the full data. Remember to scale the data (for now, ignore if you get some convergence warnings - you can try to increase the number of iterations, but this is not obligatory)  
```{r}

```
  
  ii. Plot the classification accuracies for each time sample for the analysis with PCA and for the one without in the same plot. Have time (ms) on the _x_-axis and classification accuracy on the _y_-axis 
```{r}

```
  
  iii. Describe the differences between the two analyses - focus on the time interval between 0 ms and 400 ms - describe in your own words why the logistic regression performs better on the PCA-reduced dataset around the peak magnetic activity
```{r}

```
