---
title: "Methods 3 - Assignment 1"
author: "Esteban David Maidana"
date: "09/08/2022"
output:
  pdf_document: default
  word_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Required Packages 
## install.packages("rethinking") ##Installed
## install.packages("slider") ##Installed
## install.packages("plotly") ##Installed
## install.packages("dplyr") ##Installed
## install.packages("ggplot2") ##Installed
## install.packages("skimr") ##Installed
## install.packages("XQuartz") ##Installed
## install.packages("fastmap") ##Installed
## install.packages('knitr', repos='http://cran.rstudio.org') ##Installed

## Required libraries
library(rethinking) ##Loaded
library(tidyverse) ##Loaded
library(dplyr) ##Loaded
library(ggplot2) ##Loaded
library(skimr) ##Loaded
library(lme4) ##Loaded
library(data.table) ##Loaded
library(nlme) ##Loaded
library(MASS) ## Loaded

## LOAD DATA
## Get working directory
setwd('/Users/estebandavidmaidana/Desktop')
working_directory = getwd()
```
## Assignment 1: Using mixed effects modelling to model hierarchical data (Winter & Grawunder, 2012). 
In this assignment we will be investigating the _politeness_ dataset of Winter and Grawunder (2012) and apply basic methods of multilevel modelling. 

## Dataset
The dataset has been shared on GitHub, so make sure that the csv-file is on your current path. Otherwise you can supply the full path.
```{r}
## LOAD DATA
## Get working directory
setwd('/Users/estebandavidmaidana/Desktop')
working_directory = getwd()

## Load data from csv file
file = paste(working_directory, sep = "", "/working_directory/politeness.csv")
politeness <- read.csv(file) ## Read in data
```
## Exercises and objectives:
The objectives of the exercises of this assignment are:  
1) Learning to recognize hierarchical structures within datasets and describing them  
2) Creating simple multilevel models and assessing their fitness  
3) Write up a report about the findings of the study  

## REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below.  

## REMEMBER: This assignment will be part of your final portfolio.

## Exercise 1: Describing the dataset and making some initial plots.
1) Describe the dataset, such that someone who happened upon this dataset could understand the variables and what they contain  
  i. Also consider whether any of the variables in _politeness_ should be encoded as factors or have the factor encoding removed. Hint: ```?factor```
```{r}
politeness[1:4] = lapply(politeness[1:4], factor)
```
## Answer: Defaults to encoding subject, gender, attitude as characters and the rest as numeric. We should encode subject, gender, scenario, and attitude as factors.

## Answer: There are several variables here, subject appears to be an ID variable for each subject, gender is M or F, scenario presumably refers to different testing conditions, attitude seems to have the options polite or informal. Duration is duration in some unit, the others are unclear.

2) Create a new data frame that just contains the subject _F1_ and run two linear models; one that expresses _f0mn_ as dependent on _scenario_ as an integer; and one that expresses _f0mn_ as dependent on _scenario_ encoded as a factor  
  i. Include the model matrices, $X$ from the General Linear Model, for these two models in your report and describe the different interpretations of _scenario_ that these entail
  ii. Which coding of _scenario_, as a factor or not, is more fitting? You want to code it as a factor, presumably, since it's a category of scenario.
```{r}
## Creating new data frame that has just the subject F1.
politeness_f1 <- politeness %>%
  filter(subject == 'F1')

## Factor version of linear model.
f1lm_factor <- glm(f0mn~factor(scenario),data = politeness_f1)
f1lm_int <- glm(f0mn~as.integer(scenario),data = politeness_f1)

## Model matrices.
fact_matrix <- model.matrix(f1lm_factor)
int_matrix <- model.matrix(f1lm_int)

fact_matrix
int_matrix
```
## Answer: For the factor matrix, it treats "scenario" as a series of binary dummy variables, taking the 7 levels and breaking that down into 6 separate dummy variables. For the integer matrix, it instead treats "scenario" as an integer ranking from 1 to 7, this being a single variable which indicates which of the 7 scenarios was utilized. The implication of the integer version is that it would have some ordinality (an order to it, where e.g. scenario 7 was "greater" in some way than scenario 1) where with the factor version each scenario is treated equally. In a linear regression model, the effect this has is that for the integer version, there is a signle slope (beta) assigned to all of the scenarios, where in the factor version there is a different beta value for each scenario, which essentially shows up in the model as a shifting of the y-intercept calculated independently for each value for "scenario".

3) Make a plot that includes a subplot for each subject that has _scenario_ on the x-axis and _f0mn_ on the y-axis and where points are colour coded according to _attitude_
  i. Describe the differences between subjects:
```{r}
ggplot(drop_na(politeness), aes(as.integer(scenario), f0mn, color = attitude))+
  geom_point()+
  geom_line()+
  labs(x='Scenario', y = 'f0mn')+
  theme_bw()+
  facet_wrap(~subject)
```
## Answer: There are multiple major differences between subjects - notably, different subjects experience different shifts in f0mn based on different scenarios. There is not a noticeable, consistent trend across subjects in how their f0mn levels respond to various scenarios. Furthermore, each subject has a different baseline f0mn. Some subjects have a mean f0mn in the low 100's, where others are in the upper 200's, to highlight this difference I've made a summary plot below.

```{r}
politeness %>%
  drop_na %>%
  select(f0mn, scenario, attitude, subject)%>%
  group_by(subject, attitude)%>%
  summarise(f0mn_dev = sd(f0mn),f0mn = mean(f0mn))%>%
  ggplot(aes(x= subject, fill = attitude, y = f0mn))+
  geom_col(position = "dodge")+
  ## errorbars look ugly, but do confirm statistically significant difference
  ## geom_errorbar(aes(ymin = f0mn-f0mn_dev, ymax = f0mn+f0mn_dev))+
theme_bw()
```
## Answer: Clearly there is a significant difference between the average f0mn for different subjects, which should be accounted for if looking for a treatment effect. 
    
## Exercise 2: Comparison of models.
For this part, make sure to have `lme4` installed.  
You can install it using `install.packages("lme4")` and load it using `library(lme4)`  
`lmer` is used for multilevel modelling
```{r, eval=FALSE}
library(lme4) ## Loaded

## mixed.model <- lmer(formula=..., data=...)
## example.formula <- formula(dep.variable ~ first.level.variable + (1 | second.level.variable))
```

1) Build four models and do some comparisons
  i. a single level model that models _f0mn_ as dependent on _gender_
```{r}
single <- lm(f0mn ~ gender, data = politeness)
```

  ii. a two-level model that adds a second level on top of i. where unique intercepts are modelled for each _scenario_
```{r}
scenario_twolvl <- lmer(f0mn ~ gender + (1|scenario), data = politeness)
```

  iii. a two-level model that only has _subject_ as an intercept
```{r}
subject_twolvl <- lmer(f0mn ~ gender + (1|subject), data = politeness)
```

  iv. a two-level model that models intercepts for both _scenario_ and _subject_
```{r}
both_twolvl <- lmer(f0mn ~ gender + (1|subject) + (1|scenario), data = politeness)
```
  
  v. which of the models has the lowest residual standard deviation, also compare the Akaike Information Criterion `AIC`?
## Comment: To check for residual standard deviation, we can run summary on each of these models 
```{r}
summary(single)
## RSD = 39.46
summary(scenario_twolvl)
## RSD = 38.448
summary(subject_twolvl)
## RSD = 32.04
summary(both_twolvl)
## RSD = 30.658
```
## Answer: The model with the lowest residual standard deviation is the two-level model which includes both _subject_ and _scenario_. 

```{r}
AIC(single)
## 2163.97
AIC(scenario_twolvl)
## 2152.31
AIC(subject_twolvl)
## 2099.63
AIC(both_twolvl)
## 2092.48

## Answer: Lowest AIC in the model that includes both second-level effects.
```
## Answer: The lowest AIC is found in the model which includes gender and both subject and scenario. Ideally we want the lowest possible AIC value, so we would include gender as a first-level effect, and subject and scenario as second-level effect variables in our model. The variable which reduces AIC most with its inclusion is subject. 

  vi. which of the second-level effects explains the most variance?
## Answer: The second-level effect of "subject" describes the most variance, as can be seen by its impact on AIC relative to scenario as well as the reduction of RSD that occurs after its inclusion relative to that of scenario. 
    
2) Why is our single-level model bad?
## Answer: Our single level model is too simple, only including gender and not accounting for the subject level differences or the scenario differences. The single level model only assumes that there will be a flat difference between f0mn and 

  i. create a new data frame that has three variables, _subject_, _gender_ and _f0mn_, where _f0mn_ is the average of all responses of each subject, i.e. averaging across _attitude_ and_scenario_
```{r}
subgen_politeness <- politeness %>%
  group_by(subject, gender) %>%
  summarize(f0mn = mean(f0mn, na.rm = TRUE))

subgen_politeness
```

  ii. build a single-level model that models _f0mn_ as dependent on _gender_ using this new dataset
```{r}
single_summary <-lm(f0mn ~ gender, data = subgen_politeness)
```

  iii. make Quantile-Quantile plots, comparing theoretical quantiles to the sample quantiles) using `qqnorm` and `qqline` for the new single-level model and compare it to the old single-level model (from 1).i). Which model's residuals ($\epsilon$) fulfil the assumptions of the General Linear Model better?)
## General linear model assumptions:
## - The data are independently distributed.
## - Normal residuals.
## - Homoscedasticity

```{r}
## qq plot of our new lm model
qqnorm(resid(single_summary))
qqline(resid(single_summary))

## hist(resid(single_summary))

## qq plot of our old lm model
qqnorm(resid(single))
qqline(resid(single))

## hist(resid(single))
```
## Answer: The new model that is operating on summary data does appear to do a better job of meeting the assumptions of the general linear model, notably the reduction of the size of tails at the ends suggests that the data is less skewed in its summarized form. This means that normality of residuals has improved, and therefore the new model better satisfies the "normality of residuals" assumption.

  iv. Also make a quantile-quantile plot for the residuals of the  multilevel model with two intercepts. Does it look alright?
```{r}
qqnorm(resid(both_twolvl))
qqline(resid(both_twolvl))

## Comment: This looks okay but could be better, with some tails at the end that deviate. Still it isn't bad.
```
## Answer: Yes this looks alright, however it is notable that there are some fairly pronounced tails, particularly at the positive end, and therefore the distribution of residuals is right-skewed. This doesn't fully violate the assumption of normally-distributed residuals, but it could be better. 

3) Plotting the two-intercepts model
  i. Create a plot for each subject, (similar to part 3 in Exercise 1), this time also indicating the fitted value for each of the subjects for each of the scenarios (hint use `fixef` to get the "grand effects" for each gender and `ranef` to get the subject- and scenario-specific effects)
```{r}
library(data.table) ## Loaded
library(nlme) ## Loaded
fixef(both_twolvl)
ranef(both_twolvl)

## First I made a dataframe containing the value contributed by the subject.
test <- data.frame(subj_int = ranef(both_twolvl)[1])
rownames(test)

test2 <- data.frame(test, subject = rownames(test)) 

## Here I added a gender column from the subject names (there are lots of better ways to do this).
test2$gender = "M"
for (i in 1:nrow(test2)) {
  if ("F1" %in% test2$subject[i] |"F2" %in% test2$subject[i] |"F3" %in% test2$subject[i] |"F4" %in% test2$subject[i] |"F5" %in% test2$subject[i] |"F6" %in% test2$subject[i] |"F7" %in% test2$subject[i] |"F8" %in% test2$subject[i] |"F9" %in% test2$subject[i]) {
    test2$gender[i] = "F"
  }
}

## Renaming to subgender (for subject and gender).
subgender <- test2

## Repeating. This time making a dataframe by scenario.
test <- data.frame(ranef(both_twolvl)[2])

test2 <- data.frame(test, scenario = rownames(test)) 
test2 <- rename(test2, scenario.int = X.Intercept.)

## Here I'm making a for loop where it spits out a dataframe with repeats of the scenarios, each with a different subject name next to it.
test3 <- test2
test3$subject = NA
test4 = test3[0,1:3]
for (i in 1:nrow(subgender)) {
  test3$subject = subgender$subject[i]
  test4 = rbind(test4, test3)
}

scenarios = test4

## This allowed me to right join them, by subject.
combined <-right_join(subgender,scenarios, by = 'subject')
combined <- rename(combined,subj.int = X.Intercept.)

## Finally, took the combined dataset, and created a gender integer column based off of the fixed effects of gender.
full <- combined %>%
  mutate(gender.int = ifelse(gender == "F", 246.7650, 131.5904)) %>%
  mutate(predicted = subj.int + gender.int + scenario.int)

ggplot(full, aes(scenario, predicted))+
  geom_point()+
  labs(y = 'predicted f0mn')+
  facet_wrap(~subject)
```
## Exercise 3: Now with attitude.
1) Carry on with the model with the two unique intercepts fitted (_scenario_ and _subject_).
  i. now build a model that has _attitude_ as a main effect besides _gender_
```{r}
att_gend_lmer <- lmer(f0mn ~ gender + attitude + (1|subject) + (1|scenario), data = politeness)
```
  
  ii. make a separate model that besides the main effects of _attitude_ and _gender_ also include their interaction
```{r}
att_gend_int_lmer <- lmer(f0mn ~ gender + attitude + attitude*gender + (1|subject) + (1|scenario), data = politeness)

summary(att_gend_int_lmer)
```
  iii. describe what the interaction term in the model says about Korean men's pitch when they are polite relative to Korean women's pitch when they are polite (you don't have to judge whether it is interesting) 
## Answer: This suggests that Korean men shift their pitch down by 5.5 units less relative to women when speaking politely.

2) Compare the three models (1. gender as a main effect; 2. gender and attitude as main effects; 3. gender and attitude as main effects and the interaction between them. For all three models model unique intercepts for _subject_ and _scenario_) using residual variance, residual standard deviation and AIC. 
```{r}
summary(both_twolvl)
## resid var: 939.92  resid sd: 30.658 
summary(att_gend_lmer)
## resid var: 882.7    resid sd: 29.71 
summary(att_gend_int_lmer)
## resid var: 885.5    resid sd: 29.76 


AIC(both_twolvl)
## 2092.49
AIC(att_gend_lmer)
## 2077.13
AIC(att_gend_int_lmer)
## 2072.62

summary_stats <- data.frame(model = c("main: gender", "main: gender + attitude", "main: gender + attitude, int: gender*attitude"),residual_variance = c(939.92,882.7,885.5), residual_sd = c(30.66, 29.71, 29.76), AIC = c(2092.49,2077.13,2072.62))

print(summary_stats)
```
## Answer: Judging off of AIC, the lowest AIC value is the largest model which includes both intercepts, attitude, gender, and their interaction. This is lower than that without the interaction by 5 units. Residual variance and standard deviation are very slightly higher for the model that includes the interaction, but both are still significantly lower than in the model which only included gender as a main effect. This decision is somewhat arbitrary, but given the slightly lower AIC and barely higher residual variance and standard deviation, I would select the model including the interaction effect. 

3)  Choose the model that you think describe the data the best - and write a short report on the main findings based on this model. At least include the following:
  i. describe what the dataset consists of
## Answer: 
## The dataset consists of 7 variables:
## - The subject's ID,
## - The subject's gender,
## - The scenario which is a categorical variable corresponding to the type of interaction (ie. "making an appointment", "asking a favor" p. 809 of the Winter and Grawunder paper),
## - The attitude the subject was portraying (polite or informal),
## - The total duration of the audio,
## - f0mn, which is a measure of pitch,
## - And hiss_count which is a count of the number of verbal 'hisses' in the audio.

  ii. what can you conclude about the effect of gender and attitude on pitch (if anything)?  
## Answer: Being male led to a lower pitch (expectedly) by about -118 units and being polite led to a lower pitch by about -17 units for women and -12 units for men.

  iii. motivate why you would include separate intercepts for subjects and scenarios (if you think they should be included)
## Answer: Clearly it is necessary for each of the subjects to have separate intercepts, as they will each have a different base pitch based on their voice. Scenario is a little less clear, but it is possible that subtle differences in scenario could lead to slight differences in pitch, which would be nice to control for when the goal is primarily to note the effect of politeness.

  iv. describe the variance components of the second level (if any)  
## Answer: There was a high degree of subject variance (expected) and a lower degree of scenario variance. There was still a fair amount of unexplained variance as noted by the variance of the residuals, some of which was explained by the main effects. Though this model is also not perfect, as can be seen by the Q-Q plot of the residuals below which has a tail.
  
  v. include a Quantile-Quantile plot of your chosen model  
```{r}
qqnorm(resid(att_gend_int_lmer))
qqline(resid(att_gend_int_lmer))
```
